{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95e4e58c",
   "metadata": {},
   "source": [
    "## Question 1: What is Simple Linear Regression (SLR)? Explain its purpose.\n",
    "\n",
    "**Answer (Expanded and Theoretical):**\n",
    "\n",
    "Simple Linear Regression (SLR) is a statistical and machine learning technique used to model the relationship between two continuous variables: one independent (predictor) variable `X` and one dependent (response) variable `Y`. The model assumes this relationship is linear and can be expressed as a straight line through the data. SLR belongs to the family of **parametric** models because it assumes a specific functional form (linear) and estimates a small number of parameters (two in SLR).\n",
    "\n",
    "### Key theoretical points:\n",
    "- **Model form:** The model can be written as\n",
    "\n",
    "  \\[ Y = \\beta_0 + \\beta_1 X + \\epsilon \\]\n",
    "\n",
    "  where `β₀` is the intercept, `β₁` is the slope, and `ε` is a random error term representing unexplained variability.\n",
    "\n",
    "- **Assumptions:** (see Question 2 for details) — the correctness of inference and many statistical properties hinge on assumptions like linearity, homoscedasticity, independence and normality of errors.\n",
    "\n",
    "- **Estimation goal:** Estimate `β₀` and `β₁` from observed data so that the fitted line best explains the observed `Y` values.\n",
    "\n",
    "- **Statistical interpretation:** The slope `β₁` quantifies the average change in `Y` for one-unit change in `X`, holding other factors constant (there are no other factors in SLR).\n",
    "\n",
    "- **Uses and purpose:**\n",
    "  - **Prediction:** Estimate `Y` for new values of `X`.\n",
    "  - **Inference:** Test hypotheses (e.g., is `β₁ = 0`?) and build confidence intervals for parameters.\n",
    "  - **Explanation:** Understand direction and strength of association between `X` and `Y`.\n",
    "  - **Baseline modeling:** SLR often acts as a simple baseline before progressing to multiple regression or non-linear methods.\n",
    "\n",
    "- **Limitations:** Limited to linear relationships; sensitive to outliers and influential points; real-world phenomena often require multiple predictors or non-linear forms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd85341",
   "metadata": {},
   "source": [
    "## Question 2: What are the key assumptions of Simple Linear Regression?\n",
    "\n",
    "**Answer (Detailed):**\n",
    "\n",
    "SLR relies on several assumptions. Violations affect estimator properties, hypothesis tests, and predictive performance. Below are the assumptions, their meanings, diagnostics and consequences of violation:\n",
    "\n",
    "1. **Linearity**\n",
    "   - **Meaning:** The conditional expectation of `Y` given `X` is a linear function of `X`: `E[Y|X] = β₀ + β₁X`.\n",
    "   - **Diagnostic:** Scatter plots of `Y` vs `X`, residual vs fitted plots.\n",
    "   - **Consequence if violated:** Coefficients become biased for the true relationship; consider polynomial or non-linear models.\n",
    "\n",
    "2. **Independence of errors**\n",
    "   - **Meaning:** Error terms `ε_i` are uncorrelated; `Cov(ε_i, ε_j) = 0` for `i ≠ j`.\n",
    "   - **Diagnostic:** Durbin-Watson test, autocorrelation plots (ACF) especially for time series.\n",
    "   - **Consequence if violated:** Standard errors are incorrect, leading to invalid confidence intervals and hypothesis tests.\n",
    "\n",
    "3. **Homoscedasticity (constant variance)**\n",
    "   - **Meaning:** `Var(ε_i) = σ²` for all observations.\n",
    "   - **Diagnostic:** Plot residuals vs fitted values; Breusch-Pagan test.\n",
    "   - **Consequence if violated (heteroscedasticity):** OLS estimates remain unbiased but are no longer the Best Linear Unbiased Estimators (BLUE); standard errors are biased.\n",
    "\n",
    "4. **Normality of errors**\n",
    "   - **Meaning:** `ε_i ~ N(0, σ²)` (for inference and small-sample properties).\n",
    "   - **Diagnostic:** Q-Q plot of residuals, Shapiro-Wilk test.\n",
    "   - **Consequence if violated:** Confidence intervals and p-values may be unreliable for small samples; with large samples, the Central Limit Theorem mitigates this.\n",
    "\n",
    "5. **No perfect multicollinearity**\n",
    "   - **Meaning:** In SLR this is trivial (one predictor). For multiple regression, predictors should not be exact linear combinations of each other.\n",
    "\n",
    "6. **Exogeneity / No omitted variable bias**\n",
    "   - **Meaning:** The predictor `X` should be uncorrelated with the error term: `Cov(X, ε) = 0`.\n",
    "   - **Consequence if violated:** Coefficient estimates are biased and inconsistent. This commonly occurs with omitted confounders or measurement errors in `X`.\n",
    "\n",
    "**Practical notes:** Diagnostics and remedial measures (transformations, robust standard errors, generalized least squares, or additional predictors) are used when assumptions do not hold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a3d1df",
   "metadata": {},
   "source": [
    "## Question 3: Write the mathematical equation for a simple linear regression model and explain each term.\n",
    "\n",
    "**Answer (Expanded):**\n",
    "\n",
    "The standard SLR equation is:\n",
    "\n",
    "\\[ Y = \\beta_0 + \\beta_1 X + \\epsilon \\]\n",
    "\n",
    "**Term-by-term explanation:**\n",
    "- **Y (dependent variable):** The outcome variable we want to predict or explain.\n",
    "- **X (independent variable):** The predictor variable used to explain variation in `Y`.\n",
    "- **β₀ (intercept):** The expected value of `Y` when `X = 0`; geometrically, the point where the regression line crosses the Y-axis.\n",
    "- **β₁ (slope coefficient):** The expected change in `Y` for a one-unit increase in `X` (average marginal effect).\n",
    "- **ε (error term / disturbance):** A random variable capturing the deviation of observed `Y` from the deterministic part `β₀ + β₁X`. It represents unobserved factors, measurement error, and inherent randomness.\n",
    "\n",
    "**Additional notes:**\n",
    "- The model implies `E[Y|X] = β₀ + β₁X` and `Var(Y|X) = Var(ε) = σ²` under homoscedasticity.\n",
    "- Estimation of `β₀` and `β₁` is commonly done using Ordinary Least Squares (OLS)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1aca75",
   "metadata": {},
   "source": [
    "## Question 4: Provide a real-world example where simple linear regression can be applied.\n",
    "\n",
    "**Answer (Detailed Example): Predicting House Prices by Size**\n",
    "\n",
    "**Context:** Real estate analysts often want to model the relationship between a house's sale price and a single measurable attribute such as living area (square feet).\n",
    "\n",
    "- **Dependent variable (Y):** House sale price (e.g., in ₹ or $).\n",
    "- **Independent variable (X):** Living area (square feet).\n",
    "\n",
    "**Why SLR fits:** There is typically an approximately linear relationship: larger houses tend to sell for higher prices. A linear model gives a simple interpretable rate of change (price per square foot).\n",
    "\n",
    "**What to check and extend:**\n",
    "- Perform exploratory data analysis and check scatter plots for linearity.\n",
    "- Check for influential outliers (extremely large homes or luxury properties) that can distort the slope.\n",
    "- If necessary, extend to multiple regression by adding predictors such as location, number of bedrooms, age of house, to reduce omitted variable bias.\n",
    "\n",
    "**Other real-world SLR examples:**\n",
    "- Predicting sales from advertising budget.\n",
    "- Estimating crop yield from rainfall.\n",
    "- Predicting student test scores from hours studied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f0012c",
   "metadata": {},
   "source": [
    "## Question 5: What is the method of least squares in linear regression?\n",
    "\n",
    "**Answer (Theory + Formulas):**\n",
    "\n",
    "The **Ordinary Least Squares (OLS)** method chooses parameters `β₀` and `β₁` that minimize the sum of squared residuals (errors). The residual for observation `i` is `e_i = Y_i - \\hat{Y_i}` where `\\hat{Y_i} = β̂₀ + β̂₁ X_i`.\n",
    "\n",
    "**Objective function:**\n",
    "\n",
    "\\[ SSE(β_0, β_1) = \\sum_{i=1}^n (Y_i - β_0 - β_1 X_i)^2 \\]\n",
    "\n",
    "The OLS estimates `β̂₀` and `β̂₁` are the values that minimize this SSE. By calculus, setting derivatives to zero gives closed-form solutions:\n",
    "\n",
    "\\[ β̂_1 = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2} \\]\n",
    "\\[ β̂_0 = \\bar{Y} - β̂_1 \\bar{X} \\]\n",
    "\n",
    "**Properties of OLS (under Gauss-Markov conditions):**\n",
    "- **Unbiasedness:** `E[β̂] = β`.\n",
    "- **Efficiency:** Among all linear unbiased estimators, OLS has the smallest variance (BLUE) if errors are homoscedastic and uncorrelated.\n",
    "- **Variance formula:** `Var(β̂_1) = σ² / Σ(X_i - X̄)²` and `Var(β̂_0) = σ² (1/n + X̄²/Σ(X_i - X̄)²)`.\n",
    "\n",
    "**Interpretation:** OLS minimizes vertical distances (errors in `Y`), not perpendicular distances to the line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abe0b6f",
   "metadata": {},
   "source": [
    "## Question 6: What is Logistic Regression? How does it differ from Linear Regression?\n",
    "\n",
    "**Answer (Thorough):**\n",
    "\n",
    "Logistic Regression is a generalized linear model (GLM) used for binary classification. Instead of modelling the conditional mean `E[Y|X]` directly as a linear function, logistic regression models the **log-odds** (logit) of the probability that `Y = 1` as a linear function of predictors:\n",
    "\n",
    "\\[ \\text{logit}(P(Y=1|X)) = \\log\\frac{P(Y=1|X)}{1 - P(Y=1|X)} = β_0 + β_1 X \\]\n",
    "\n",
    "**Equivalently,** the probability is given by the logistic (sigmoid) function:\n",
    "\n",
    "\\[ P(Y=1|X) = \\frac{1}{1 + e^{-(β_0 + β_1 X)}}. \\]\n",
    "\n",
    "**Key differences vs Linear Regression:**\n",
    "- **Outcome type:** Linear → continuous; Logistic → binary/categorical.\n",
    "- **Model target:** Linear models `E[Y|X]`; logistic models `P(Y=1|X)` via a transformed (logit) scale.\n",
    "- **Estimation:** Linear uses OLS; logistic uses maximum likelihood estimation (MLE).\n",
    "- **Interpretation of coefficients:** In logistic regression, `e^{β_1}` is the odds ratio for a one-unit increase in `X`.\n",
    "\n",
    "**Use cases:** Spam detection, credit default prediction, medical diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df836010",
   "metadata": {},
   "source": [
    "## Question 7: Name and briefly describe three common evaluation metrics for regression models.\n",
    "\n",
    "**Answer (Expanded with interpretation):**\n",
    "\n",
    "1. **Mean Absolute Error (MAE):**\n",
    "   - Formula: \\( MAE = \\frac{1}{n} \\sum_{i=1}^n |Y_i - \\hat{Y}_i| \\).\n",
    "   - Interpretation: Average absolute prediction error in the same units as `Y`. Less sensitive to outliers than MSE.\n",
    "\n",
    "2. **Mean Squared Error (MSE):**\n",
    "   - Formula: \\( MSE = \\frac{1}{n} \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 \\).\n",
    "   - Interpretation: Penalizes larger errors more heavily; useful for optimization and theoretical analysis.\n",
    "\n",
    "3. **Root Mean Squared Error (RMSE):**\n",
    "   - Formula: \\( RMSE = \\sqrt{MSE} \\).\n",
    "   - Interpretation: Presents error on the same scale as `Y`. Easier to interpret than MSE.\n",
    "\n",
    "**Additional metrics (brief):**\n",
    "- **Mean Absolute Percentage Error (MAPE):** Useful when relative errors matter, but problematic if `Y` can be zero.\n",
    "- **R-squared:** Proportion of variance explained (covered in Question 8)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646dbffa",
   "metadata": {},
   "source": [
    "## Question 8: What is the purpose of the R-squared metric in regression analysis?\n",
    "\n",
    "**Answer (Detailed):**\n",
    "\n",
    "R-squared (R²) quantifies the proportion of variance in the dependent variable `Y` that is explained by the independent variable(s) `X` in the model. Formally:\n",
    "\n",
    "\\[ R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} \\]\n",
    "\n",
    "where `SS_res = Σ(Y_i - \\hat{Y}_i)^2` and `SS_tot = Σ(Y_i - \\bar{Y})^2`.\n",
    "\n",
    "**Interpretation and caveats:**\n",
    "- `R² = 0` → model explains none of the variance; `R² = 1` → perfect fit.\n",
    "- R² increases with additional predictors; use **Adjusted R²** to penalize model complexity.\n",
    "- High R² doesn't imply causation or model correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41974ff9",
   "metadata": {},
   "source": [
    "## Question 9: Write Python code to fit a simple linear regression model using scikit-learn and print the slope and intercept.\n",
    "\n",
    "**Answer (Code + Explanation):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40ec84a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope (β₁): 0.8100\n",
      "Intercept (β₀): 1.7700\n",
      "MSE: 0.0038\n",
      "RMSE: 0.0616\n",
      "R-squared: 0.9971\n"
     ]
    }
   ],
   "source": [
    "# Scikit-learn example for Simple Linear Regression (SLR)\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Sample data (X: feature, y: target)\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "y = np.array([2.5, 3.5, 4.2, 5.0, 5.8])\n",
    "\n",
    "# Fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Coefficients\n",
    "slope = model.coef_[0]\n",
    "intercept = model.intercept_\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "print(f\"Slope (β₁): {slope:.4f}\")\n",
    "print(f\"Intercept (β₀): {intercept:.4f}\")\n",
    "\n",
    "# Additional evaluation\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19beb01",
   "metadata": {},
   "source": [
    "**Explanation:** The `LinearRegression` model fits weights using OLS. `model.coef_` returns slope(s) and `model.intercept_` returns intercept. The extra metrics show model fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e20db23",
   "metadata": {},
   "source": [
    "## Question 10: How do you interpret the coefficients in a simple linear regression model?\n",
    "\n",
    "**Answer (Expanded):**\n",
    "\n",
    "Given `Y = β₀ + β₁ X + ε`:\n",
    "\n",
    "- **Intercept (β₀):** The expected value of `Y` when `X = 0`. In practice, assess whether `X = 0` is in the domain; if not, the intercept may not have meaningful real-world interpretation.\n",
    "\n",
    "- **Slope (β₁):** The expected change in `Y` given a one-unit increase in `X`. It captures direction (positive/negative) and magnitude of association.\n",
    "\n",
    "**Statistical inference:**\n",
    "- Use standard errors, t-tests and confidence intervals to assess whether coefficients are statistically significantly different from zero.\n",
    "- A small p-value for `β₁` suggests a statistically significant association between `X` and `Y` under model assumptions.\n",
    "\n",
    "**Practical considerations:**\n",
    "- Check residual diagnostics, influence measures (e.g., Cook's distance), and multicollinearity (in multiple regression) before trusting coefficient interpretation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
