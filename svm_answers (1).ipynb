{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa643795",
   "metadata": {},
   "source": [
    "## Question 1: What is Information Gain, and how is it used in Decision Trees?\n",
    "\n",
    "**Answer (theory):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fd6af7",
   "metadata": {},
   "source": [
    "Information Gain is a metric from information theory used to measure the reduction in uncertainty about a target variable after observing a feature. In decision trees, nodes are split by choosing features that maximize Information Gain — the idea being that a good split should result in child nodes that are purer (i.e., more homogeneous with respect to class labels) than their parent. Numerically, Information Gain is computed as the difference between the entropy of the parent node and the weighted sum of entropies of the child nodes. Entropy itself quantifies impurity or disorder: high entropy means classes are mixed. When constructing a decision tree, the algorithm evaluates candidate splits for each feature, computes the Information Gain for each, and selects the split that yields the highest gain. This greedy approach builds trees top-down: the most informative features are used near the root. Information Gain favors features that partition the data well, but can be biased toward features with many distinct values; techniques like gain ratio are sometimes used to correct this bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ee5924",
   "metadata": {},
   "source": [
    "## Question 2: What is the difference between Gini Impurity and Entropy?\n",
    "\n",
    "**Answer (theory):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d79187d",
   "metadata": {},
   "source": [
    "Gini Impurity and Entropy are two impurity measures used to evaluate the quality of splits in classification decision trees. Entropy (from information theory) measures the average amount of information needed to identify the class of a randomly drawn sample, and is maximized when classes are equally probable. Gini Impurity measures the probability of misclassifying a randomly chosen sample if it were labeled according to the class distribution in that node. In practice both prefer purer splits and often select similar splits, but they differ numerically and have distinct properties: entropy grows logarithmically and is more sensitive to changes in probabilities near 0 or 1, while Gini is simpler and computationally cheaper (no logarithms). Gini tends to produce slightly different splits and can be faster in large datasets. Entropy (Information Gain) is theoretically grounded in information theory and may be preferable when interpreting splits in terms of information. In many real-world tasks the difference in accuracy is minor; choice is often driven by algorithm implementation, interpretability, or computational constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6071b3f9",
   "metadata": {},
   "source": [
    "## Question 3: What is Pre-Pruning in Decision Trees?\n",
    "\n",
    "**Answer (theory):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca07a78",
   "metadata": {},
   "source": [
    "Pre-pruning (also called early stopping) is a technique used during the construction of decision trees to halt tree growth before the tree fully fits the training data. The goal is to avoid overfitting — when a tree becomes overly complex, modeling noise rather than underlying patterns. Pre-pruning imposes constraints like maximum depth, minimum samples required to split a node, minimum samples per leaf, maximum number of leaf nodes, or a threshold on impurity decrease. During tree construction, if a candidate split does not satisfy these constraints or does not yield a sufficient improvement in impurity, the split is rejected and the node remains a leaf. Pre-pruning reduces model complexity, can improve generalization, and speeds up training. However, it risks underfitting if constraints are too strict. Selecting appropriate pre-pruning hyperparameters typically requires cross-validation. Compared to post-pruning (which first builds a full tree and then prunes nodes), pre-pruning avoids constructing very large trees but may miss beneficial deeper structure that would be revealed after further splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c624bf",
   "metadata": {},
   "source": [
    "## Question 4: Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba01733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importances (Iris dataset):\n",
      "Feature 0: 0.0133\n",
      "Feature 1: 0.0000\n",
      "Feature 2: 0.5641\n",
      "Feature 3: 0.4226\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Q4: Decision Tree with Gini Impurity - feature importances\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "clf.fit(X, y)\n",
    "importances = clf.feature_importances_\n",
    "print(\"Feature importances (Iris dataset):\")\n",
    "for i, imp in enumerate(importances):\n",
    "    print(f\"Feature {i}: {imp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c462f7",
   "metadata": {},
   "source": [
    "## Question 5: What is a Support Vector Machine (SVM)?\n",
    "\n",
    "**Answer (theory):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cf931f",
   "metadata": {},
   "source": [
    "A Support Vector Machine (SVM) is a supervised learning algorithm primarily used for classification (and regression) tasks. SVM seeks to find the optimal decision boundary (hyperplane) that separates classes with the maximum margin — where margin is the distance between the hyperplane and the nearest data points of any class, called support vectors. Maximizing the margin often improves generalization on unseen data. For linearly separable data, SVM finds the unique maximum-margin hyperplane; for non-separable or noisy data, SVM uses slack variables to allow some misclassifications and introduces a regularization parameter (C) to balance margin size against misclassification penalties. For non-linear problems SVMs are extended with kernels that implicitly map inputs into higher-dimensional feature spaces where a linear separator may exist. Popular kernels include linear, polynomial, and radial basis function (RBF). SVMs are robust to high-dimensional spaces, can work well with clear margin separation, and rely on a subset of training points (support vectors), making them memory efficient for many datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c7a172",
   "metadata": {},
   "source": [
    "## Question 6: What is the Kernel Trick in SVM?\n",
    "\n",
    "**Answer (theory):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b46301",
   "metadata": {},
   "source": [
    "The Kernel Trick is a technique that enables Support Vector Machines to perform non-linear classification without explicitly mapping data into a higher-dimensional space. Instead of computing the coordinates of data in an expanded feature space, the kernel trick computes inner products between pairs of data points in that feature space using a kernel function. Because many algorithms (including SVM) can be expressed in terms of dot products between samples, replacing the dot product with a kernel function implicitly performs the mapping. Common kernel functions include the linear kernel, polynomial kernel, and the Radial Basis Function (RBF) kernel. For example, RBF computes similarity based on distance and corresponds to an infinite-dimensional feature mapping. The kernel trick allows SVMs to learn complex decision boundaries while avoiding the computational cost of explicit transformations. Choosing an appropriate kernel and its hyperparameters is crucial; it determines the shape of the decision boundary and influences bias–variance trade-offs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ad4221",
   "metadata": {},
   "source": [
    "## Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2b765c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM accuracy: 0.9556\n",
      "RBF SVM accuracy:    0.7111\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Q7: SVM with Linear and RBF kernels on Wine dataset - compare accuracies\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_wine(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "svm_lin = SVC(kernel='linear', random_state=42)\n",
    "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "svm_lin.fit(X_train, y_train)\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lin = svm_lin.predict(X_test)\n",
    "y_pred_rbf = svm_rbf.predict(X_test)\n",
    "\n",
    "acc_lin = accuracy_score(y_test, y_pred_lin)\n",
    "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
    "\n",
    "print(f\"Linear SVM accuracy: {acc_lin:.4f}\")\n",
    "print(f\"RBF SVM accuracy:    {acc_rbf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a491c3d",
   "metadata": {},
   "source": [
    "## Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
    "\n",
    "**Answer (theory):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae059385",
   "metadata": {},
   "source": [
    "The Naïve Bayes classifier is a probabilistic supervised learning method based on Bayes’ theorem. It models the posterior probability of classes given the features by combining prior class probabilities with the likelihood of features under each class. The 'naïve' part stems from the strong assumption that all features are conditionally independent given the class label — an assumption rarely true in real data. Despite this simplification, Naïve Bayes performs surprisingly well in many domains (especially text classification) because it requires relatively few training samples, learns quickly, and handles high-dimensional input efficiently. During prediction, the model computes the product of likelihoods for each feature (or sum of log-likelihoods for numerical stability) and multiplies by the class prior; the class with the highest posterior probability is selected. The conditional independence assumption reduces computational complexity and allows straightforward updates for streaming data, but when features are heavily correlated, performance may degrade. Still, its simplicity and interpretability make Naïve Bayes a strong baseline classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b09164",
   "metadata": {},
   "source": [
    "## Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes\n",
    "\n",
    "**Answer (theory):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be972709",
   "metadata": {},
   "source": [
    "Gaussian, Multinomial, and Bernoulli Naïve Bayes are specialized variants tailored to different data types and likelihood assumptions. Gaussian Naïve Bayes assumes continuous features are normally distributed within each class; the likelihood for each feature is modeled using a Gaussian with class-specific mean and variance. It's appropriate for real-valued inputs (e.g., sensor measurements). Multinomial Naïve Bayes models feature counts (non-negative integers) and assumes features follow a multinomial distribution; it is widely used in text classification with bag-of-words where features are word counts or term frequencies. Bernoulli Naïve Bayes models binary-valued features (presence/absence); it uses a Bernoulli distribution per feature and is useful when only whether a feature occurs matters rather than how often (e.g., binary word occurrence). Each variant uses Bayes’ theorem but differs in the likelihood calculation to match data characteristics; choosing the correct variant improves performance and aligns model assumptions with the input data type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf663cd",
   "metadata": {},
   "source": [
    "## Question 10: Breast Cancer Dataset — Train Gaussian Naïve Bayes and evaluate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b39e28c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB accuracy on Breast Cancer dataset: 0.9371\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Q10: Gaussian Naive Bayes on Breast Cancer dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"GaussianNB accuracy on Breast Cancer dataset: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8867c6b4-7fee-45c4-82ba-e437cf1eceb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
